{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Data extraction notebook\n","\n","This notebook was used to format the data according to the WS method of GlossBERT.\n"],"metadata":{"id":"yAJRQULga4rv"}},{"cell_type":"code","source":["!pip install transformers\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ysZJ22IbPLY","executionInfo":{"status":"ok","timestamp":1675343826044,"user_tz":-60,"elapsed":30714,"user":{"displayName":"F.A. Leistra","userId":"17579912925770168307"}},"outputId":"f8ebc731-3c25-48e8-f464-d0771ae0b9c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/fa/33/acfd230f5c3e7d19bfae949dca45c230fbf1d4d6f62a0b2365caac17c37a/tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[0m  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/huggingface-hub/\u001b[0m\u001b[33m\n","\u001b[0mCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miJlgzGEfryt","executionInfo":{"status":"ok","timestamp":1675343994912,"user_tz":-60,"elapsed":213,"user":{"displayName":"F.A. Leistra","userId":"17579912925770168307"}},"outputId":"ced2ac5e-a5f5-4d3d-903a-cd37c6987663"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from nltk.toolbox import to_settings_string\n","import pandas as pd\n","from nltk.corpus import wordnet\n","from nltk.corpus.reader.wordnet import WordNetError\n","part = 'test'\n","data_path = f'/data/only_sns/{part}'\n","with open(data_path, 'r') as f:\n","    raw_sent = \"\"\n","    sents = []\n","    sns_values = []\n","    labels = []\n","    offsets = []\n","    concat = []\n","    errors = []\n","    uniques = 0\n","    for line in f:\n","        # check if line starts with 'raw sent ='\n","        if line.startswith('# raw sent ='):\n","            raw_sent = line.strip().split('=')[1].strip()\n","        elif line.startswith('#'):\n","            continue\n","        else:\n","            # split line by tab and get the second column (sns value)\n","            split_row = line.strip().split('\\t')\n","            if len(split_row) > 1:\n","              tok, sns = split_row[0], split_row[1]\n","            if sns != \"O\":\n","              # Get all wordnet synsets\n","              synsets = wordnet.synsets(tok)\n","              # we only want to store sentences for which we have at least 4 senses\n","              if len(synsets) >= 4:\n","                # store the correct sysnet\n","                WS_sent = raw_sent.replace(tok, f'\"{tok}\"')\n","                try:\n","                  wn_synset = wordnet.synset(sns)\n","                except WordNetError as e:\n","                  errors.append((tok, sns))\n","                  continue\n","                correct_def = wn_synset.definition()\n","                uniques += 1\n","                \n","                sents.append(WS_sent)\n","                sns_values.append(f'{tok}: {correct_def}')\n","                labels.append('Yes')\n","                concat.append(f'{WS_sent} [SEP] {correct_def}')\n","                offsets.append(wn_synset.offset())\n","\n","                # retrieve 3 different senses\n","                for s in synsets[:4]:\n","                  definition = s.definition()\n","                  if definition != correct_def:\n","                    sents.append(WS_sent)\n","                    sns_values.append(f'{tok}: {definition}')\n","                    labels.append('No')\n","                    concat.append(f'{WS_sent} [SEP] {tok}: {definition}')\n","                    offsets.append(s.offset())\n","\n","            # reset the sentence and continue\n","            if line.strip() == \".\":\n","                raw_sent = \"\"\n","\n","\n","df = pd.DataFrame({\"sent\": sents, \"sns\": sns_values, \"labels\": labels, \"input\": concat, \"offset\": offsets})\n","\n","print(df.head())\n","print(f'Done processing {part}, Total sentences: {len(sents)}, Unique sentences: {len(set(sents))}')\n","print(f'During processing, we encountered {len(errors)} tokens for which we could not retrieve the WordNet definition')\n","print(f'{uniques} Unique Context-Gloss pairs created')\n","store_path = f'/data/processed/{part}.csv'\n","df.to_csv(store_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6HlVB9KbEyR","executionInfo":{"status":"ok","timestamp":1675344220073,"user_tz":-60,"elapsed":453,"user":{"displayName":"F.A. Leistra","userId":"17579912925770168307"}},"outputId":"50f00153-949b-41fa-d08d-aeb3216374ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                               sent  \\\n","0  A animal is \"grazing\" in a field   \n","1  A animal is \"grazing\" in a field   \n","2  A animal is \"grazing\" in a field   \n","3  A animal is \"grazing\" in a field   \n","4  A animal is grazing in a \"field\"   \n","\n","                                                 sns labels  \\\n","0            grazing: feed as in a meadow or pasture    Yes   \n","1                        grazing: the act of grazing     No   \n","2  grazing: the act of brushing against while pas...     No   \n","3  grazing: break the skin (of a body part) by sc...     No   \n","4  field: a piece of land cleared of trees and us...    Yes   \n","\n","                                               input   offset  \n","0  A animal is \"grazing\" in a field [SEP] feed as...  1576165  \n","1  A animal is \"grazing\" in a field [SEP] the act...   841091  \n","2  A animal is \"grazing\" in a field [SEP] the act...   150762  \n","3  A animal is \"grazing\" in a field [SEP] break t...  1608508  \n","4  A animal is grazing in a \"field\" [SEP] a piece...  8569998  \n","Done processing test, Total sentences: 7938, Unique sentences: 1854\n","During processing, we encountered 2 tokens for which we could not retrieve the WordNet definition\n","1867 Unique Context-Gloss pairs created\n"]}]}]}